{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-11T22:19:37.853190Z",
     "start_time": "2025-09-11T22:19:37.835918Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import MyNeuralNet\n",
    "import neural_net_optimized\n",
    "from lr_utils import load_dataset\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:30:56.298376Z",
     "start_time": "2025-09-11T18:30:56.288325Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()",
   "id": "e2b77b834e003436",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:30:56.312150Z",
     "start_time": "2025-09-11T18:30:56.308285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x_orig_shape = train_x_orig.shape\n",
    "train_y_shape = train_y.shape\n",
    "test_x_orig_shape = test_x_orig.shape\n",
    "test_y_shape = test_y.shape\n",
    "\n",
    "print(f\"train_x_orig shape: {train_x_orig.shape} \\ntrain_y shape: {train_y.shape} \\ntest_x_orig shape: {test_x_orig.shape} \\ntest_y shape: {test_y.shape}\")"
   ],
   "id": "3076604d6f1d227f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig shape: (209, 64, 64, 3) \n",
      "train_y shape: (1, 209) \n",
      "test_x_orig shape: (50, 64, 64, 3) \n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:30:56.355189Z",
     "start_time": "2025-09-11T18:30:56.349156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reshape the training array such that each column represents an image\n",
    "train_x_orig_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_orig_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "print(f\"shape of train_x_orig_flatten = {train_x_orig_flatten.shape}, \\nshape of test_x_orig_flatten = {test_x_orig_flatten.shape}\")"
   ],
   "id": "460dd8e86f98c1d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_x_orig_flatten = (12288, 209), \n",
      "shape of test_x_orig_flatten = (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:30:56.417084Z",
     "start_time": "2025-09-11T18:30:56.404168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standardize the results, ensuring to fit the learned scalar from the training set to the test set.\n",
    "train_X = train_x_orig_flatten/255.\n",
    "test_X = test_x_orig_flatten/255.\n",
    "\n",
    "print(f\"Shape of train_X: {train_X.shape} \\nShape of test_X: {test_X.shape}\")"
   ],
   "id": "2c6966512c8d034",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (12288, 209) \n",
      "Shape of test_X: (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:31:45.737362Z",
     "start_time": "2025-09-11T18:30:56.467513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let us build 2 NN's, a 2-layer NN and an L-layer NN, and compare there performance\n",
    "def neural_network(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = MyNeuralNet.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation(X)\n",
    "\n",
    "        # Use the prediction to compute the cost\n",
    "        cost = NN.compute_cost(AL, Y)\n",
    "\n",
    "        # perform back prop, initializing it with AL and Y.\n",
    "        grads = NN.backward_propagation(AL, Y, caches)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters(grads, learning_rate)\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN\n",
    "\n",
    "layers_dims = [12288, 7, 1]\n",
    "costs, two_layer_NN = neural_network(train_X, train_y, layers_dims, print_cost = True)"
   ],
   "id": "df7b2f48d1a40227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.7074426632736273\n",
      "Cost after iteration 500: 0.3336897138438556\n",
      "Cost after iteration 1000: 0.12412486168658173\n",
      "Cost after iteration 1500: 0.05362667055856123\n",
      "Cost after iteration 2000: 0.03330274414524972\n",
      "Cost after iteration 2500: 0.02306424025583977\n",
      "Cost after iteration 2999: 0.016151946179173104\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:31:45.762455Z",
     "start_time": "2025-09-11T18:31:45.747555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the two layer NN.\n",
    "predictions_train = two_layer_NN.predict(train_X)\n",
    "predictions_test = two_layer_NN.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 2 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "print(f\"2 layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "b47d28ddbb30ea6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 74.00%\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:32:50.714859Z",
     "start_time": "2025-09-11T18:31:45.770694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN = neural_network(train_X, train_y, layers_dims, print_cost = True)"
   ],
   "id": "18d155af423c47d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.208125010600323\n",
      "Cost after iteration 500: 0.647319447526937\n",
      "Cost after iteration 1000: 0.6374058448197395\n",
      "Cost after iteration 1500: 0.6243486119913269\n",
      "Cost after iteration 2000: 0.5391408405511141\n",
      "Cost after iteration 2500: 0.2998273840124648\n",
      "Cost after iteration 2999: 0.08237974441376025\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:32:50.769251Z",
     "start_time": "2025-09-11T18:32:50.754340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN.predict(train_X)\n",
    "predictions_test = four_layer_NN.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")\n",
    "\n",
    "# We can observe there is a large difference between our training and test set error. This means our model\n",
    "# Currently has high variance. To try to combat this, we can try and accumulate more data, or try regularization."
   ],
   "id": "754ed230b45eb7b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 80.00%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:32:50.796187Z",
     "start_time": "2025-09-11T18:32:50.791673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create a L-layer neural network with L2 and dropout regularization\n",
    "def neural_network_reg(X, Y, layers_dims, lamda=0, keep_prob=1, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results.\n",
    "\n",
    "    NN = MyNeuralNet.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting.\n",
    "\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation_with_dropout(X, keep_prob)\n",
    "\n",
    "        # Use the prediction to compute the cost.\n",
    "        cost = NN.compute_cost_reg(AL, Y, lamda)\n",
    "\n",
    "        # Perform back prop.\n",
    "        grads = NN.backward_propagation_with_dropout(AL, Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters(grads, learning_rate)\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true.\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "f8782d8cf75486f6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:34:05.864941Z",
     "start_time": "2025-09-11T18:32:50.804171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN.\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN_reg = neural_network_reg(train_X, train_y, layers_dims, lamda=0.4, keep_prob=0.9, print_cost = True, reg = True)"
   ],
   "id": "3ec7401bec905f6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.3363956407504674\n",
      "Cost after iteration 500: 0.7133243064748825\n",
      "Cost after iteration 1000: 0.6114794757911202\n",
      "Cost after iteration 1500: 0.3263759060098015\n",
      "Cost after iteration 2000: 0.21516703390282282\n",
      "Cost after iteration 2500: 0.1541758841485032\n",
      "Cost after iteration 2999: 0.15980388508192295\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:34:06.005109Z",
     "start_time": "2025-09-11T18:34:05.981102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN_reg.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN.\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "3a512c52e4de4454",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 84.00%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:34:06.035820Z",
     "start_time": "2025-09-11T18:34:06.029893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By using dropout and l2 regularization we have managed to increase the test_accuracy slightly.\n",
    "# To further improve on this we could do a hyperparameter search over the learning rate, keep_prob, and lamda value\n",
    "# to find a more optimal configuration of these parameters. We can also see at iteration 2999 the cost has increased\n",
    "# slightly this could mean our learning rate is too large at the end, to combat this we could implement learning rate decay\n",
    "# or use the Adam optimizer which adapts the learning rate during training."
   ],
   "id": "910bc2bfec5b66ab",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:34:06.050573Z",
     "start_time": "2025-09-11T18:34:06.042514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create a L-layer neural network with L2 and dropout regularization that uses the Adam optimizer.\n",
    "def neural_network_reg_adam(X, Y, layers_dims, lamda=0, keep_prob=1, beta_1=0.9, beta_2=0.999, learning_rate = 0.0075, num_iterations = 2000, print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = MyNeuralNet.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "        t = i + 1\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation_with_dropout(X, keep_prob)\n",
    "\n",
    "        # Use the prediction to compute the cost\n",
    "        cost = NN.compute_cost_reg(AL, Y, lamda)\n",
    "\n",
    "        # Perform back prop\n",
    "        grads = NN.backward_propagation_with_dropout(AL, Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters_adam(grads, learning_rate, t, beta_1, beta_2)\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "5a59a3b65262e4a9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:35:17.337695Z",
     "start_time": "2025-09-11T18:34:06.063031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN_reg_adam = neural_network_reg_adam(train_X, train_y, layers_dims,\n",
    "                                                        lamda=0.0008, keep_prob=0.9,\n",
    "                                                        beta_1=0.9, beta_2=0.999, learning_rate = 0.00005,\n",
    "                                                        print_cost=True, reg=True)"
   ],
   "id": "d81396ddf85b4cef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.2661729845409995\n",
      "Cost after iteration 500: 0.5985790532733838\n",
      "Cost after iteration 1000: 0.4806575932217139\n",
      "Cost after iteration 1500: 0.4617297991324054\n",
      "Cost after iteration 1999: 0.4387438885154836\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:35:17.418946Z",
     "start_time": "2025-09-11T18:35:17.403207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN_reg_adam.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg_adam.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "46387482310dc57c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 86.00%\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T18:35:17.446478Z",
     "start_time": "2025-09-11T18:35:17.442181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By reducing the number of iterations (another form of regularization called early stopping)\n",
    "# and implementing Adam optimization we have managed to squeeze an extra 2% from the model.\n",
    "# Too further improve the model we could perform hyperparameter tuning to find a more optimal config\n",
    "# of hyperparameters, we could collect more data, or create our own using data augmentation. We could\n",
    "# also improve the speed of our algorithm by using Adam on mini-batches of our training data instead\n",
    "# of our entire dataset at once. We could also use batch-norm to further speed up our learning speed whilst\n",
    "# increasing the stability of our network by giving the NN the flexibility to change the mean and variance\n",
    "# of the neurons."
   ],
   "id": "17dfcbf2209e10d3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:20:25.593078Z",
     "start_time": "2025-09-11T22:20:25.583988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create a L-layer neural network with L2 and dropout regularization that uses the Adam optimizer.\n",
    "def neural_network_reg_adam_batch_norm(X, Y, layers_dims, lamda=0, keep_prob=1, beta_1=0.9, beta_2=0.999, learning_rate = 0.0075, num_iterations = 1000, print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = neural_net_optimized.NeuralNetOptimized(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    t = 0\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "        # Create mini_batches out of our X and Y datasets.\n",
    "        mini_batches = NN.create_mini_batches(X, Y, mini_batch_size=64)\n",
    "        epoch_cost = 0\n",
    "\n",
    "        # Iterate through each mini_batch\n",
    "        for mini_batch in mini_batches:\n",
    "            mini_batch_X, mini_batch_Y = mini_batch\n",
    "\n",
    "            t = t + 1\n",
    "            # Extract prediction from forward prop and caches from each layer.\n",
    "            AL, caches = NN.forward_propagation_with_dropout(mini_batch_X, keep_prob)\n",
    "\n",
    "            # Use the prediction to compute the cost\n",
    "            cost = NN.compute_cost_reg(AL, mini_batch_Y, lamda)\n",
    "\n",
    "            # Perform back prop\n",
    "            grads = NN.backward_propagation_with_dropout(AL, mini_batch_Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "            # Update parameter using gradient descent.\n",
    "            NN.update_parameters_adam(grads, learning_rate, t, beta_1, beta_2)\n",
    "\n",
    "            epoch_cost += cost\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true\n",
    "        if (i % 100 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            avg_cost = epoch_cost / len(mini_batches)\n",
    "            print(f\"Cost after epoch {i}: {avg_cost}\")\n",
    "            costs.append(avg_cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "11f1ef73b36e7162",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:21:24.039294Z",
     "start_time": "2025-09-11T22:20:26.109623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN_reg_batch = neural_network_reg_adam_batch_norm(train_X, train_y, layers_dims,\n",
    "                                                        lamda=2.7, keep_prob=0.9,\n",
    "                                                        beta_1=0.9, beta_2=0.999, learning_rate = 0.0005,\n",
    "                                                        print_cost=True, reg=True)"
   ],
   "id": "ef10880b62d57d1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 3.439256505214925\n",
      "Cost after epoch 100: 1.057562053230881\n",
      "Cost after epoch 200: 0.7139672289961112\n",
      "Cost after epoch 300: 0.5172373236660756\n",
      "Cost after epoch 400: 0.40208635413261445\n",
      "Cost after epoch 500: 0.29244882957299245\n",
      "Cost after epoch 600: 0.23433766517055044\n",
      "Cost after epoch 700: 0.2194024331543517\n",
      "Cost after epoch 800: 0.1694090685297685\n",
      "Cost after epoch 900: 0.18010873145433148\n",
      "Cost after epoch 999: 0.10219573846698099\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T22:21:24.062505Z",
     "start_time": "2025-09-11T22:21:24.050268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN_reg_batch.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg_batch.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "cc71898e45a91946",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN: \n",
      "Accuracy_train: 93.78% \n",
      "Accuracy_test: 70.00%\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# It seems in this case using batch-norm and mini-batch hasn't helped the performance of our model.\n",
    "# This could be for numerous reasons, one is the hyperparameter search space is larger since we have\n",
    "# to search for mini_batch size, learning rate, keep_prob, lambda, # layers, # nodes etc. This can make\n",
    "# finding optimal parameters for this configuration more difficult. Also, using many forms of\n",
    "# regularization such as L2, dropout, and batch norm may be too restrictive on our model and may be\n",
    "# inhibiting it from learning the optimal weights.\n",
    "\n",
    "# In this notebook I learned how to implement He initialization and why it is important when using\n",
    "# ReLu as our primary activation function. I also learned how to generalize my code to enable and\n",
    "# account for deeper NNs. I have also learned how to implement L2 and dropout regularization\n",
    "# and how they can help reduce overfitting. I learned how to implement Adam optimization and learned how\n",
    "# having a non-fixed learning rate can help improve model performance."
   ],
   "id": "525f599124d7e34a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
