{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T19:18:17.385747Z",
     "start_time": "2025-09-09T19:18:16.236519Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import MyNeuralNet\n",
    "from lr_utils import load_dataset\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:18:17.396989Z",
     "start_time": "2025-09-09T19:18:17.390626Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()",
   "id": "e2b77b834e003436",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:18:17.406533Z",
     "start_time": "2025-09-09T19:18:17.402934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x_orig_shape = train_x_orig.shape\n",
    "train_y_shape = train_y.shape\n",
    "test_x_orig_shape = test_x_orig.shape\n",
    "test_y_shape = test_y.shape\n",
    "\n",
    "print(f\"train_x_orig shape: {train_x_orig.shape} \\ntrain_y shape: {train_y.shape} \\ntest_x_orig shape: {test_x_orig.shape} \\ntest_y shape: {test_y.shape}\")"
   ],
   "id": "3076604d6f1d227f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig shape: (209, 64, 64, 3) \n",
      "train_y shape: (1, 209) \n",
      "test_x_orig shape: (50, 64, 64, 3) \n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:18:17.425146Z",
     "start_time": "2025-09-09T19:18:17.421943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reshape the training array such that each column represents an image\n",
    "train_x_orig_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_orig_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "print(f\"shape of train_x_orig_flatten = {train_x_orig_flatten.shape}, \\nshape of test_x_orig_flatten = {test_x_orig_flatten.shape}\")"
   ],
   "id": "460dd8e86f98c1d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_x_orig_flatten = (12288, 209), \n",
      "shape of test_x_orig_flatten = (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:18:17.452257Z",
     "start_time": "2025-09-09T19:18:17.443526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standardize the results, ensuring to fit the learned scalar from the training set to the test set.\n",
    "train_X = train_x_orig_flatten/255.\n",
    "test_X = test_x_orig_flatten/255.\n",
    "\n",
    "print(f\"Shape of train_X: {train_X.shape} \\nShape of test_X: {test_X.shape}\")"
   ],
   "id": "2c6966512c8d034",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (12288, 209) \n",
      "Shape of test_X: (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:18:49.151338Z",
     "start_time": "2025-09-09T19:18:17.459594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let us build 2 NN's, a 2-layer NN and an L-layer NN, and compare there performance\n",
    "def neural_network(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = MyNeuralNet.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation(X)\n",
    "\n",
    "        # Use the prediction to compute the cost\n",
    "        cost = NN.compute_cost(AL, Y)\n",
    "\n",
    "        # perform back prop, initializing it with AL and Y.\n",
    "        grads = NN.backward_propagation(AL, Y, caches)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters(grads, learning_rate)\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN\n",
    "\n",
    "layers_dims = [12288, 7, 1]\n",
    "costs, two_layer_NN = neural_network(train_X, train_y, layers_dims, print_cost = True)"
   ],
   "id": "df7b2f48d1a40227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.7074426632736273\n",
      "Cost after iteration 500: 0.3336897138438556\n",
      "Cost after iteration 1000: 0.12412486168658173\n",
      "Cost after iteration 1500: 0.05362667055856123\n",
      "Cost after iteration 2000: 0.03330274414524972\n",
      "Cost after iteration 2500: 0.02306424025583977\n",
      "Cost after iteration 2999: 0.016151946179173104\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:18:49.164190Z",
     "start_time": "2025-09-09T19:18:49.156923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the two layer NN.\n",
    "predictions_train = two_layer_NN.predict(train_X)\n",
    "predictions_test = two_layer_NN.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 2 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "print(f\"2 layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "b47d28ddbb30ea6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 74.00%\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:19:29.775047Z",
     "start_time": "2025-09-09T19:18:49.170134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN = neural_network(train_X, train_y, layers_dims, print_cost = True)"
   ],
   "id": "18d155af423c47d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.208125010600323\n",
      "Cost after iteration 500: 0.647319447526937\n",
      "Cost after iteration 1000: 0.6374058448197395\n",
      "Cost after iteration 1500: 0.6243486119913269\n",
      "Cost after iteration 2000: 0.5391408405511141\n",
      "Cost after iteration 2500: 0.2998273840124648\n",
      "Cost after iteration 2999: 0.08237974441376025\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:19:29.793110Z",
     "start_time": "2025-09-09T19:19:29.784644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN.predict(train_X)\n",
    "predictions_test = four_layer_NN.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")\n",
    "\n",
    "# We can observe there is a large difference between our training and test set error. This means our model\n",
    "# Currently has high variance. To try to combat this, we can try and accumulate more data, or try regularization."
   ],
   "id": "754ed230b45eb7b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 80.00%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:19:29.805411Z",
     "start_time": "2025-09-09T19:19:29.801594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def neural_network_reg(X, Y, layers_dims, lamda=0, keep_prob=1, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = MyNeuralNet.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation_with_dropout(X, keep_prob)\n",
    "\n",
    "        # Use the prediction to compute the cost\n",
    "        cost = NN.compute_cost_reg(AL, Y, lamda)\n",
    "\n",
    "        # perform back prop, initializing it with AL and Y.\n",
    "        grads = NN.backward_propagation_with_dropout(AL, Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters(grads, learning_rate)\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "f8782d8cf75486f6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:23:26.982446Z",
     "start_time": "2025-09-09T19:22:40.745290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN_reg = neural_network_reg(train_X, train_y, layers_dims, lamda=0.4, keep_prob=0.9, print_cost = True, reg = True)"
   ],
   "id": "3ec7401bec905f6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.3363956407504674\n",
      "Cost after iteration 500: 0.7133243064748825\n",
      "Cost after iteration 1000: 0.6114794757911202\n",
      "Cost after iteration 1500: 0.3263759060098015\n",
      "Cost after iteration 2000: 0.21516703390282282\n",
      "Cost after iteration 2500: 0.1541758841485032\n",
      "Cost after iteration 2999: 0.15980388508192295\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:23:27.002796Z",
     "start_time": "2025-09-09T19:23:26.993402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN_reg.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "3a512c52e4de4454",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 84.00%\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:21:21.663500Z",
     "start_time": "2025-09-09T19:21:21.661003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We can see using dropout and l2 regularization we have managed a slight improvement in our test_accuracy.\n",
    "# To further improve on this we could do a hyperparameter search over the learning rate, keep_prob, and lamda value\n",
    "# to find a more optimal configuration of these parameters. We can also see at iteration 2999 the cost has increased\n",
    "# slightly this could mean our learning rate is large at the end, to combat this we could implement learning rate decay\n",
    "# or use the Adam optimizer which adapts the learning rate during training.\n",
    "\n",
    "# In this notebook I learned how to implement He initialization and why it is important when using\n",
    "# ReLu as our primary activation function. I also learned how to generalize my code to enable and\n",
    "# account for deeper NNs. I have also learned how to implement L2 and dropout regularization\n",
    "# and how they can help reduce overfitting."
   ],
   "id": "910bc2bfec5b66ab",
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
