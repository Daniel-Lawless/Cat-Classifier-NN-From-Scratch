{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-10T15:36:54.021535Z",
     "start_time": "2025-09-10T15:36:52.743760Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import MyNeuralNet\n",
    "from lr_utils import load_dataset\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:36:54.032308Z",
     "start_time": "2025-09-10T15:36:54.026078Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()",
   "id": "e2b77b834e003436",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:36:54.040811Z",
     "start_time": "2025-09-10T15:36:54.037500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x_orig_shape = train_x_orig.shape\n",
    "train_y_shape = train_y.shape\n",
    "test_x_orig_shape = test_x_orig.shape\n",
    "test_y_shape = test_y.shape\n",
    "\n",
    "print(f\"train_x_orig shape: {train_x_orig.shape} \\ntrain_y shape: {train_y.shape} \\ntest_x_orig shape: {test_x_orig.shape} \\ntest_y shape: {test_y.shape}\")"
   ],
   "id": "3076604d6f1d227f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig shape: (209, 64, 64, 3) \n",
      "train_y shape: (1, 209) \n",
      "test_x_orig shape: (50, 64, 64, 3) \n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:36:54.067855Z",
     "start_time": "2025-09-10T15:36:54.064711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reshape the training array such that each column represents an image\n",
    "train_x_orig_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_orig_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "print(f\"shape of train_x_orig_flatten = {train_x_orig_flatten.shape}, \\nshape of test_x_orig_flatten = {test_x_orig_flatten.shape}\")"
   ],
   "id": "460dd8e86f98c1d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_x_orig_flatten = (12288, 209), \n",
      "shape of test_x_orig_flatten = (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:36:54.105697Z",
     "start_time": "2025-09-10T15:36:54.096528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standardize the results, ensuring to fit the learned scalar from the training set to the test set.\n",
    "train_X = train_x_orig_flatten/255.\n",
    "test_X = test_x_orig_flatten/255.\n",
    "\n",
    "print(f\"Shape of train_X: {train_X.shape} \\nShape of test_X: {test_X.shape}\")"
   ],
   "id": "2c6966512c8d034",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (12288, 209) \n",
      "Shape of test_X: (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let us build 2 NN's, a 2-layer NN and an L-layer NN, and compare there performance\n",
    "def neural_network(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = MyNeuralNet.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation(X)\n",
    "\n",
    "        # Use the prediction to compute the cost\n",
    "        cost = NN.compute_cost(AL, Y)\n",
    "\n",
    "        # perform back prop, initializing it with AL and Y.\n",
    "        grads = NN.backward_propagation(AL, Y, caches)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters(grads, learning_rate)\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN\n",
    "\n",
    "layers_dims = [12288, 7, 1]\n",
    "costs, two_layer_NN = neural_network(train_X, train_y, layers_dims, print_cost = True)"
   ],
   "id": "df7b2f48d1a40227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the two layer NN.\n",
    "predictions_train = two_layer_NN.predict(train_X)\n",
    "predictions_test = two_layer_NN.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 2 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "print(f\"2 layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "b47d28ddbb30ea6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN = neural_network(train_X, train_y, layers_dims, print_cost = True)"
   ],
   "id": "18d155af423c47d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN.predict(train_X)\n",
    "predictions_test = four_layer_NN.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")\n",
    "\n",
    "# We can observe there is a large difference between our training and test set error. This means our model\n",
    "# Currently has high variance. To try to combat this, we can try and accumulate more data, or try regularization."
   ],
   "id": "754ed230b45eb7b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:38:02.855448Z",
     "start_time": "2025-09-10T15:38:02.850434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create a L-layer neural network with L2 and dropout regularization\n",
    "def neural_network_reg(X, Y, layers_dims, lamda=0, keep_prob=1, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results.\n",
    "\n",
    "    NN = MyNeuralNet.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting.\n",
    "\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation_with_dropout(X, keep_prob)\n",
    "\n",
    "        # Use the prediction to compute the cost.\n",
    "        cost = NN.compute_cost_reg(AL, Y, lamda)\n",
    "\n",
    "        # Perform back prop.\n",
    "        grads = NN.backward_propagation_with_dropout(AL, Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters(grads, learning_rate)\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true.\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "f8782d8cf75486f6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:38:50.013595Z",
     "start_time": "2025-09-10T15:38:02.878873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN.\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN_reg = neural_network_reg(train_X, train_y, layers_dims, lamda=0.4, keep_prob=0.9, print_cost = True, reg = True)"
   ],
   "id": "3ec7401bec905f6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.3363956407504674\n",
      "Cost after iteration 500: 0.7133243064748825\n",
      "Cost after iteration 1000: 0.6114794757911202\n",
      "Cost after iteration 1500: 0.3263759060098015\n",
      "Cost after iteration 2000: 0.21516703390282282\n",
      "Cost after iteration 2500: 0.1541758841485032\n",
      "Cost after iteration 2999: 0.15980388508192295\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:38:50.052109Z",
     "start_time": "2025-09-10T15:38:50.043671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN_reg.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN.\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "3a512c52e4de4454",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 84.00%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:35:53.050774Z",
     "start_time": "2025-09-10T15:35:53.047510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# By using dropout and l2 regularization we have managed to increase the test_accuracy slightly.\n",
    "# To further improve on this we could do a hyperparameter search over the learning rate, keep_prob, and lamda value\n",
    "# to find a more optimal configuration of these parameters. We can also see at iteration 2999 the cost has increased\n",
    "# slightly this could mean our learning rate is too large at the end, to combat this we could implement learning rate decay\n",
    "# or use the Adam optimizer which adapts the learning rate during training."
   ],
   "id": "910bc2bfec5b66ab",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T19:01:01.397758Z",
     "start_time": "2025-09-10T19:01:01.393150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to create a L-layer neural network with L2 and dropout regularization that uses the Adam optimizer.\n",
    "def neural_network_reg_adam(X, Y, layers_dims, lamda=0, keep_prob=1, beta_1=0.9, beta_2=0.999, learning_rate = 0.0075, num_iterations = 2000, print_cost=False, reg=False):\n",
    "\n",
    "    np.random.seed(1) # Define np seed for reproducible results\n",
    "\n",
    "    NN = MyNeuralNet.NeuralNet(layers_dims) # Create NN instance, which initializes parameters.\n",
    "\n",
    "    costs = [] # Collection of costs for plotting\n",
    "\n",
    "    # repeat forward prop, back prop, and gradient descent num_iterations times.\n",
    "    for i in range(num_iterations):\n",
    "        t = i + 1\n",
    "        # Extract prediction from forward prop and caches from each layer.\n",
    "        AL, caches = NN.forward_propagation_with_dropout(X, keep_prob)\n",
    "\n",
    "        # Use the prediction to compute the cost\n",
    "        cost = NN.compute_cost_reg(AL, Y, lamda)\n",
    "\n",
    "        # Perform back prop\n",
    "        grads = NN.backward_propagation_with_dropout(AL, Y, caches, keep_prob, lamda, reg)\n",
    "\n",
    "        # Update parameter using gradient descent.\n",
    "        NN.update_parameters_adam(grads, learning_rate, t, beta_1, beta_2)\n",
    "\n",
    "        # Print cost per 500 iterations if print_cost is true\n",
    "        if (i % 500 == 0 or i == num_iterations - 1) and print_cost == True:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    return costs, NN"
   ],
   "id": "5a59a3b65262e4a9",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T19:01:46.388611Z",
     "start_time": "2025-09-10T19:01:01.613964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set structure for the 4 layer NN\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "\n",
    "# Train model.\n",
    "costs, four_layer_NN_reg_adam = neural_network_reg_adam(train_X, train_y, layers_dims,\n",
    "                                                        lamda=0.0008, keep_prob=0.9,\n",
    "                                                        beta_1=0.9, beta_2=0.999, learning_rate = 0.00005,\n",
    "                                                        print_cost=True, reg=True)"
   ],
   "id": "d81396ddf85b4cef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.2661729845409995\n",
      "Cost after iteration 500: 0.5985790532733838\n",
      "Cost after iteration 1000: 0.4806575932217139\n",
      "Cost after iteration 1500: 0.4617297991324054\n",
      "Cost after iteration 1999: 0.4387438885154836\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T19:01:46.414220Z",
     "start_time": "2025-09-10T19:01:46.403521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions for train and test with the four layer NN.\n",
    "predictions_train = four_layer_NN_reg_adam.predict(train_X)\n",
    "predictions_test = four_layer_NN_reg_adam.predict(test_X)\n",
    "\n",
    "# Select the first row of each array to make them 1D. Calculate accuracy for 4 layer NN\n",
    "accuracy_train = accuracy_score(predictions_train[0], train_y[0])\n",
    "accuracy_test = accuracy_score(predictions_test[0], test_y[0])\n",
    "\n",
    "print(np.squeeze(predictions_train))\n",
    "\n",
    "print(f\"{len(layers_dims) - 1} layer NN: \\nAccuracy_train: {100 * accuracy_train:.2f}% \\nAccuracy_test: {100 * accuracy_test:.2f}%\")"
   ],
   "id": "46387482310dc57c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      "4 layer NN: \n",
      "Accuracy_train: 100.00% \n",
      "Accuracy_test: 86.00%\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# By reducing the number of iterations (another form of regularization called early stopping)\n",
    "# and implementing Adam optimization we have managed to squeeze an extra 2% from the model.\n",
    "# Too further improve the model we could perform hyperparameter tuning to find a more optimal config\n",
    "# of hyperparameters, we could collect more data, or create our own using data augmentation. We could\n",
    "# also improve the speed of our algorithm by using Adam on mini-batches of our training data instead\n",
    "# of our entire dataset at once. We could also use batch-norm to further speed up our learning speed whilst\n",
    "# increasing the stability of our network by giving the NN the flexibility to change the mean and variance\n",
    "# of the neurons. By increasing the speed of training, we can try more hyperparameters in a shorter timeframe.\n",
    "\n",
    "# In this notebook I learned how to implement He initialization and why it is important when using\n",
    "# ReLu as our primary activation function. I also learned how to generalize my code to enable and\n",
    "# account for deeper NNs. I have also learned how to implement L2 and dropout regularization\n",
    "# and how they can help reduce overfitting. I learned how to implement Adam optimization and learned how\n",
    "# having a non-fixed learning rate can help improve model performance."
   ],
   "id": "17dfcbf2209e10d3",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
